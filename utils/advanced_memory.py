"""
Advanced Memory Management Î³Î¹Î± Î¼ÎµÎ³Î¬Î»Î± scRNA-seq datasets

Î‘Ï…Ï„ÏŒ Ï„Î¿ module Ï€ÎµÏÎ¹Î»Î±Î¼Î²Î¬Î½ÎµÎ¹ Ï€ÏÎ¿Î·Î³Î¼Î­Î½ÎµÏ‚ Ï„ÎµÏ‡Î½Î¹ÎºÎ­Ï‚ Î³Î¹Î± Ï„Î·Î½ Î±Ï€Î¿Î´Î¿Ï„Î¹ÎºÎ®
Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î¼Î½Î®Î¼Î·Ï‚ ÏƒÎµ Î¼ÎµÎ³Î¬Î»Î± datasets, ÏƒÏ…Î¼Ï€ÎµÏÎ¹Î»Î±Î¼Î²Î±Î½Î¿Î¼Î­Î½Î¿Ï… streaming,
chunking, ÎºÎ±Î¹ progressive loading.


Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±: 2025
"""

import streamlit as st
import pandas as pd
import numpy as np
import scanpy as sc
import gc
import psutil
import warnings
from typing import Optional, Tuple, List, Dict, Any
from functools import lru_cache
import tempfile
import os
from scipy import sparse
import time

class AdvancedMemoryManager:
    """Î ÏÎ¿Î·Î³Î¼Î­Î½Î· Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ· Î¼Î½Î®Î¼Î·Ï‚ Î³Î¹Î± scRNA-seq datasets"""
    
    def __init__(self):
        self.memory_threshold_mb = 500  # ÎœÎµÎ¹Ï‰Î¼Î­Î½Î¿ threshold Î³Î¹Î± Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î±
        self.chunk_size = 500  # ÎœÎ¹ÎºÏÏŒÏ„ÎµÏÎ± chunks Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î´Î¹Î±Ï‡ÎµÎ¯ÏÎ¹ÏƒÎ·
        self.cache_enabled = True
        self.streaming_mode = False
        self.max_file_size_mb = 1000  # ÎœÎ­Î³Î¹ÏƒÏ„Î¿ Î¼Î­Î³ÎµÎ¸Î¿Ï‚ Î±ÏÏ‡ÎµÎ¯Î¿Ï…
        self.emergency_cleanup_threshold = 0.85  # 85% memory usage
        
    def get_system_memory_info(self) -> Dict[str, float]:
        """Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î»ÎµÏ€Ï„Î¿Î¼ÎµÏÎµÎ¯Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î¼Î½Î®Î¼Î·Ï‚ ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î¿Ï‚"""
        memory = psutil.virtual_memory()
        process = psutil.Process()
        
        return {
            'total_gb': memory.total / (1024**3),
            'available_gb': memory.available / (1024**3),
            'used_gb': memory.used / (1024**3),
            'percent': memory.percent,
            'process_mb': process.memory_info().rss / (1024**2),
            'process_percent': process.memory_percent()
        }
    
    def should_use_streaming(self, adata) -> bool:
        """ÎšÎ±Î¸Î¿ÏÎ¯Î¶ÎµÎ¹ Î±Î½ Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î·Î¸ÎµÎ¯ streaming mode"""
        if adata is None:
            return False
            
        # Î•ÎºÏ„Î¯Î¼Î·ÏƒÎ· Î¼ÎµÎ³Î­Î¸Î¿Ï…Ï‚ ÏƒÎµ Î¼Î½Î®Î¼Î·
        estimated_size_mb = self.estimate_memory_usage(adata)
        memory_info = self.get_system_memory_info()
        
        # Î§ÏÎ®ÏƒÎ· streaming Î±Î½:
        # 1. Î¤Î¿ dataset ÎµÎ¯Î½Î±Î¹ >500MB
        # 2. Î— Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î· Î¼Î½Î®Î¼Î· ÎµÎ¯Î½Î±Î¹ <2GB
        # 3. Î¤Î¿ dataset ÎºÎ±Ï„Î±Î»Î±Î¼Î²Î¬Î½ÎµÎ¹ >30% Ï„Î·Ï‚ ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ®Ï‚ Î¼Î½Î®Î¼Î·Ï‚
        return (estimated_size_mb > 500 or 
                memory_info['available_gb'] < 2 or
                estimated_size_mb > memory_info['total_gb'] * 1024 * 0.3)
    
    def estimate_memory_usage(self, adata) -> float:
        """Î•ÎºÏ„Î¯Î¼Î·ÏƒÎ· Ï‡ÏÎ®ÏƒÎ·Ï‚ Î¼Î½Î®Î¼Î·Ï‚ ÏƒÎµ MB"""
        if adata is None:
            return 0
            
        # Î’Î±ÏƒÎ¹ÎºÎ® ÎµÎºÏ„Î¯Î¼Î·ÏƒÎ· Î³Î¹Î± X matrix
        if sparse.issparse(adata.X):
            # Sparse matrix: nnz * (4 bytes Î³Î¹Î± value + 4 bytes Î³Î¹Î± index)
            x_size = adata.X.nnz * 8
        else:
            # Dense matrix
            x_size = adata.X.nbytes
        
        # Î•ÎºÏ„Î¯Î¼Î·ÏƒÎ· Î³Î¹Î± obs, var, obsm, varm
        obs_size = adata.obs.memory_usage(deep=True).sum()
        var_size = adata.var.memory_usage(deep=True).sum()
        
        # Î•ÎºÏ„Î¯Î¼Î·ÏƒÎ· Î³Î¹Î± obsm, varm (PCA, embeddings ÎºÎ»Ï€.)
        obsm_size = sum(arr.nbytes for arr in adata.obsm.values() if hasattr(arr, 'nbytes'))
        varm_size = sum(arr.nbytes for arr in adata.varm.values() if hasattr(arr, 'nbytes'))
        
        total_bytes = x_size + obs_size + var_size + obsm_size + varm_size
        return total_bytes / (1024**2)  # Convert to MB
    
    def optimize_adata_for_memory(self, adata):
        """Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· AnnData object Î³Î¹Î± Î¼Î½Î®Î¼Î·"""
        if adata is None:
            return adata
            
        st.info("ğŸ”§ Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î³Î¹Î± Î¼Î½Î®Î¼Î·...")
        
        # 1. ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ sparse Î±Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹
        if not sparse.issparse(adata.X):
            sparsity = (adata.X == 0).sum() / adata.X.size
            if sparsity > 0.7:  # Î‘Î½ >70% zeros
                st.info(f"ğŸ“¦ ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ sparse format (sparsity: {sparsity:.1%})")
                adata.X = sparse.csr_matrix(adata.X)
        
        # 2. ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î¬Ï‡ÏÎ·ÏƒÏ„Ï‰Î½ raw data Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½
        if hasattr(adata, 'raw') and adata.raw is not None:
            if adata.raw.X.shape == adata.X.shape:
                st.info("ğŸ—‘ï¸ Î‘Ï†Î±Î¯ÏÎµÏƒÎ· duplicate raw data")
                adata.raw = None
        
        # 3. Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· data types
        self._optimize_dtypes(adata)
        
        # 4. Garbage collection
        gc.collect()
        
        return adata
    
    def _optimize_dtypes(self, adata):
        """Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¯Î·ÏƒÎ· data types Î³Î¹Î± obs/var DataFrames"""
        
        for df_name in ['obs', 'var']:
            df = getattr(adata, df_name)
            
            for col in df.columns:
                if df[col].dtype == 'object':
                    # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® ÏƒÎµ category Î±Î½ Î­Ï‡ÎµÎ¹ Î»Î¯Î³ÎµÏ‚ unique Ï„Î¹Î¼Î­Ï‚
                    unique_ratio = df[col].nunique() / len(df[col])
                    if unique_ratio < 0.5:
                        df[col] = df[col].astype('category')
                elif df[col].dtype == 'float64':
                    # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® float64 ÏƒÎµ float32 Î±Î½ Î´ÎµÎ½ Ï‡Î¬Î½ÎµÏ„Î±Î¹ Î±ÎºÏÎ¯Î²ÎµÎ¹Î±
                    if df[col].max() < np.finfo(np.float32).max:
                        df[col] = df[col].astype('float32')
                elif df[col].dtype == 'int64':
                    # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® int64 ÏƒÎµ int32 Î±Î½ ÎµÎ¯Î½Î±Î¹ Î´Ï…Î½Î±Ï„ÏŒ
                    if df[col].max() < np.iinfo(np.int32).max and df[col].min() > np.iinfo(np.int32).min:
                        df[col] = df[col].astype('int32')
    
    def create_chunked_iterator(self, adata, chunk_size: int = None):
        """Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± iterator Î³Î¹Î± chunked processing"""
        if chunk_size is None:
            chunk_size = self.chunk_size
            
        n_chunks = (adata.n_obs + chunk_size - 1) // chunk_size
        
        for i in range(n_chunks):
            start_idx = i * chunk_size
            end_idx = min((i + 1) * chunk_size, adata.n_obs)
            
            chunk = adata[start_idx:end_idx, :].copy()
            yield i, chunk, n_chunks
    
    def safe_display_data(self, adata, max_cells: int = 100, max_genes: int = 100):
        """Î‘ÏƒÏ†Î±Î»Î®Ï‚ ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï‡Ï‰ÏÎ¯Ï‚ memory overflow"""
        
        if adata is None:
            return pd.DataFrame({"Error": ["No data loaded"]})
        
        # Î ÎµÏÎ¹Î¿ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î³Î¹Î± ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ·
        display_cells = min(max_cells, adata.n_obs)
        display_genes = min(max_genes, adata.n_vars)
        
        try:
            # Î•Î¾Î±Î³Ï‰Î³Î® Î¼Î¹ÎºÏÎ¿Ï subset
            subset = adata[:display_cells, :display_genes]
            
            if sparse.issparse(subset.X):
                data_array = subset.X.toarray()
            else:
                data_array = subset.X
            
            # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± DataFrame Î¼Îµ Î±ÏƒÏ†Î±Î»Î® Î¼Î­Î³ÎµÎ¸Î¿Ï‚
            sample_df = pd.DataFrame(
                data_array,
                index=subset.obs_names,
                columns=subset.var_names
            )
            
            return sample_df
            
        except Exception as e:
            st.error(f"Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î·Î½ ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½: {str(e)}")
            return pd.DataFrame({"Error": [f"Display error: {str(e)}"]})
    
    def progressive_qc_calculation(self, adata):
        """Progressive Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ QC metrics Î³Î¹Î± Î¼ÎµÎ³Î¬Î»Î± datasets Î¼Îµ Î²ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î· Î±Ï€ÏŒÎ´Î¿ÏƒÎ· Î¼Î½Î®Î¼Î·Ï‚"""
        
        memory_info = self.get_system_memory_info()
        dataset_size_mb = self.estimate_memory_usage(adata)
        
        # Î“Î¹Î± Ï€Î¿Î»Ï Î¼ÎµÎ³Î¬Î»Î± datasets, Î´Î¿ÎºÎ¹Î¼Î® vectorized approach Ï€ÏÏÏ„Î±
        if dataset_size_mb > 500 and memory_info['available_gb'] > 4:
            st.info("ğŸš€ Î”Î¿ÎºÎ¹Î¼Î® vectorized QC calculation Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î±Ï€ÏŒÎ´Î¿ÏƒÎ·...")
            try:
                return self._vectorized_qc_calculation(adata)
            except Exception as e:
                st.warning(f"âš ï¸ Vectorized approach Î±Ï€Î­Ï„Ï…Ï‡Îµ: {str(e)}")
                st.info("ğŸ”„ Fallback ÏƒÎµ progressive processing...")
        
        st.info("ğŸ“Š Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ QC metrics Î¼Îµ progressive processing...")
        
        # Î ÏÎ¿ÏƒÎ±ÏÎ¼Î¿ÏƒÏ„Î¹ÎºÏŒ chunk size Î²Î¬ÏƒÎµÎ¹ Î¼Î½Î®Î¼Î·Ï‚ ÎºÎ±Î¹ Î¼ÎµÎ³Î­Î¸Î¿Ï…Ï‚ dataset
        dataset_size_mb = self.estimate_memory_usage(adata)
        
        # Î’ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î¿ Î´Ï…Î½Î±Î¼Î¹ÎºÏŒ chunk size Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î±Ï€ÏŒÎ´Î¿ÏƒÎ·
        if memory_info['available_gb'] < 2:
            chunk_size = max(1000, int(adata.n_obs / 20))  # ÎœÎµÎ³Î±Î»ÏÏ„ÎµÏÎ± chunks Î±ÎºÏŒÎ¼Î± ÎºÎ±Î¹ Î¼Îµ Î»Î¯Î³Î· Î¼Î½Î®Î¼Î·
        elif memory_info['available_gb'] < 4:
            chunk_size = max(2000, int(adata.n_obs / 10))  # ÎœÎµÏƒÎ±Î¯Î± chunks
        elif memory_info['available_gb'] < 8:
            chunk_size = max(5000, int(adata.n_obs / 5))   # ÎœÎµÎ³Î¬Î»Î± chunks
        else:
            chunk_size = max(10000, int(adata.n_obs / 3))  # Î Î¿Î»Ï Î¼ÎµÎ³Î¬Î»Î± chunks Î³Î¹Î± ÎºÎ±Î»Î® Î±Ï€ÏŒÎ´Î¿ÏƒÎ·
        
        st.info(f"ğŸ”§ Î§ÏÎ®ÏƒÎ· adaptive chunk size: {chunk_size} ÎºÏÏ„Ï„Î±ÏÎ± Î±Î½Î¬ chunk")
        
        # Pre-calculate gene patterns Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î±Ï€ÏŒÎ´Î¿ÏƒÎ·
        mt_gene_pattern = adata.var_names.str.startswith(('MT-', 'mt-', 'Mt-'))
        ribo_gene_pattern = adata.var_names.str.startswith(('RPS', 'RPL', 'rps', 'rpl'))
        
        # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ MT/Ribo genes
        n_mt_genes = mt_gene_pattern.sum()
        n_ribo_genes = ribo_gene_pattern.sum()
        
        if n_mt_genes == 0:
            st.warning("âš ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î¼Î¹Ï„Î¿Ï‡Î¿Î½Î´ÏÎ¹Î±ÎºÎ¬ Î³Î¿Î½Î¯Î´Î¹Î± Î¼Îµ Ï„Î± ÏƒÏ…Î½Î®Î¸Î· patterns")
        if n_ribo_genes == 0:
            st.warning("âš ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÏÎ¹Î²Î¿ÏƒÏ‰Î¼Î¹ÎºÎ¬ Î³Î¿Î½Î¯Î´Î¹Î± Î¼Îµ Ï„Î± ÏƒÏ…Î½Î®Î¸Î· patterns")
        
        # Initialize metrics Î¼Îµ pre-allocated arrays Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î±Ï€ÏŒÎ´Î¿ÏƒÎ·
        qc_metrics = {
            'n_genes': np.zeros(adata.n_obs, dtype=np.int32),
            'total_counts': np.zeros(adata.n_obs, dtype=np.float32),
            'pct_counts_mt': np.zeros(adata.n_obs, dtype=np.float32),
            'pct_counts_ribo': np.zeros(adata.n_obs, dtype=np.float32)
        }
        
        # Progressive calculation Î¼Îµ progress bar ÎºÎ±Î¹ memory monitoring
        progress_bar = st.progress(0)
        status_text = st.empty()
        memory_text = st.empty()
        
        n_chunks = (adata.n_obs + chunk_size - 1) // chunk_size
        
        for i in range(n_chunks):
            start_idx = i * chunk_size
            end_idx = min((i + 1) * chunk_size, adata.n_obs)
            
            # Update progress
            progress = (i + 1) / n_chunks
            progress_bar.progress(progress)
            status_text.text(f"Processing chunk {i+1}/{n_chunks} (ÎºÏÏ„Ï„Î±ÏÎ± {start_idx+1}-{end_idx})")
            
            # Memory monitoring
            current_memory = self.get_system_memory_info()
            memory_text.text(f"ğŸ’¾ Memory usage: {current_memory['percent']:.1f}% ({current_memory['process_mb']:.0f}MB)")
            
            # Emergency memory check
            if current_memory['percent'] > 90:
                st.error("âŒ ÎšÏÎ¯ÏƒÎ¹Î¼Î± Ï‡Î±Î¼Î·Î»Î® Î¼Î½Î®Î¼Î· - Î¼ÎµÎ¯Ï‰ÏƒÎ· chunk size")
                chunk_size = max(50, chunk_size // 2)
                continue
            
            try:
                # Î•Î¾Î±Î³Ï‰Î³Î® chunk Î¼Îµ copy Î³Î¹Î± Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î±
                chunk = adata[start_idx:end_idx, :].copy()
                
                # Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿Ï‚ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ metrics
                if sparse.issparse(chunk.X):
                    # Sparse matrix - optimized calculations
                    chunk_data = chunk.X.tocsr()  # Ensure CSR format Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· Î±Ï€ÏŒÎ´Î¿ÏƒÎ·
                    
                    # Basic metrics
                    chunk_counts = np.array(chunk_data.sum(axis=1)).flatten()
                    chunk_genes = np.array((chunk_data > 0).sum(axis=1)).flatten()
                    
                    # MT genes (Î¼Îµ Î±ÏƒÏ†Î±Î»Î® Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒ)
                    if n_mt_genes > 0:
                        mt_data = chunk_data[:, mt_gene_pattern]
                        mt_counts = np.array(mt_data.sum(axis=1)).flatten()
                        chunk_mt_pct = np.divide(mt_counts, chunk_counts, 
                                               out=np.zeros_like(mt_counts), 
                                               where=chunk_counts!=0) * 100
                    else:
                        chunk_mt_pct = np.zeros(len(chunk_counts))
                    
                    # Ribosomal genes (Î¼Îµ Î±ÏƒÏ†Î±Î»Î® Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒ)
                    if n_ribo_genes > 0:
                        ribo_data = chunk_data[:, ribo_gene_pattern]
                        ribo_counts = np.array(ribo_data.sum(axis=1)).flatten()
                        chunk_ribo_pct = np.divide(ribo_counts, chunk_counts, 
                                                 out=np.zeros_like(ribo_counts), 
                                                 where=chunk_counts!=0) * 100
                    else:
                        chunk_ribo_pct = np.zeros(len(chunk_counts))
                        
                else:
                    # Dense matrix
                    chunk_counts = chunk.X.sum(axis=1)
                    chunk_genes = (chunk.X > 0).sum(axis=1)
                    
                    if n_mt_genes > 0:
                        mt_counts = chunk.X[:, mt_gene_pattern].sum(axis=1)
                        chunk_mt_pct = np.divide(mt_counts, chunk_counts, 
                                               out=np.zeros_like(mt_counts), 
                                               where=chunk_counts!=0) * 100
                    else:
                        chunk_mt_pct = np.zeros(len(chunk_counts))
                    
                    if n_ribo_genes > 0:
                        ribo_counts = chunk.X[:, ribo_gene_pattern].sum(axis=1)
                        chunk_ribo_pct = np.divide(ribo_counts, chunk_counts, 
                                                 out=np.zeros_like(ribo_counts), 
                                                 where=chunk_counts!=0) * 100
                    else:
                        chunk_ribo_pct = np.zeros(len(chunk_counts))
                
                # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÏ„Î± pre-allocated arrays
                qc_metrics['n_genes'][start_idx:end_idx] = chunk_genes
                qc_metrics['total_counts'][start_idx:end_idx] = chunk_counts
                qc_metrics['pct_counts_mt'][start_idx:end_idx] = chunk_mt_pct
                qc_metrics['pct_counts_ribo'][start_idx:end_idx] = chunk_ribo_pct
                
                # Memory cleanup
                del chunk, chunk_counts, chunk_genes, chunk_mt_pct, chunk_ribo_pct
                if 'chunk_data' in locals():
                    del chunk_data
                if 'mt_data' in locals():
                    del mt_data
                if 'ribo_data' in locals():
                    del ribo_data
                gc.collect()
                
            except Exception as e:
                st.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿ chunk {i+1}: {str(e)}")
                # Fallback values
                qc_metrics['n_genes'][start_idx:end_idx] = 0
                qc_metrics['total_counts'][start_idx:end_idx] = 0
                qc_metrics['pct_counts_mt'][start_idx:end_idx] = 0
                qc_metrics['pct_counts_ribo'][start_idx:end_idx] = 0
        
        # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ progress indicators
        progress_bar.empty()
        status_text.empty()
        memory_text.empty()
        
        # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÏ„Î¿ adata.obs Î¼Îµ validation
        try:
            for metric, values in qc_metrics.items():
                adata.obs[metric] = values
            
            # Validation Ï„Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
            validation_results = {
                'n_genes': f"Min: {qc_metrics['n_genes'].min()}, Max: {qc_metrics['n_genes'].max()}, Mean: {qc_metrics['n_genes'].mean():.1f}",
                'total_counts': f"Min: {qc_metrics['total_counts'].min():.1f}, Max: {qc_metrics['total_counts'].max():.1f}, Mean: {qc_metrics['total_counts'].mean():.1f}",
                'pct_counts_mt': f"Min: {qc_metrics['pct_counts_mt'].min():.2f}%, Max: {qc_metrics['pct_counts_mt'].max():.2f}%, Mean: {qc_metrics['pct_counts_mt'].mean():.2f}%",
                'pct_counts_ribo': f"Min: {qc_metrics['pct_counts_ribo'].min():.2f}%, Max: {qc_metrics['pct_counts_ribo'].max():.2f}%, Mean: {qc_metrics['pct_counts_ribo'].mean():.2f}%"
            }
            
            st.success("âœ… QC metrics Ï…Ï€Î¿Î»Î¿Î³Î¯ÏƒÏ„Î·ÎºÎ±Î½ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")
            
            with st.expander("ğŸ“Š Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ QC Metrics"):
                for metric, stats in validation_results.items():
                    st.text(f"{metric}: {stats}")
                    
        except Exception as e:
            st.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î·Î½ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· QC metrics: {str(e)}")
        
        # Final memory cleanup
        del qc_metrics
        gc.collect()
        
        return adata
    
    def _vectorized_qc_calculation(self, adata):
        """Vectorized QC calculation Î³Î¹Î± Î¼ÎµÎ³Î¬Î»Î± datasets - Ï€Î¿Î»Ï Ï€Î¹Î¿ Î³ÏÎ®Î³Î¿ÏÎ· Î±Ï€ÏŒ chunked"""
        
        st.info("âš¡ Vectorized QC calculation - Ï€Î¿Î»Ï Î³ÏÎ®Î³Î¿ÏÎ· Î¼Î­Î¸Î¿Î´Î¿Ï‚...")
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # Pre-calculate gene patterns
            mt_gene_pattern = adata.var_names.str.startswith(('MT-', 'mt-', 'Mt-'))
            ribo_gene_pattern = adata.var_names.str.startswith(('RPS', 'RPL', 'rps', 'rpl'))
            
            n_mt_genes = mt_gene_pattern.sum()
            n_ribo_genes = ribo_gene_pattern.sum()
            
            status_text.text("ğŸ”¢ Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ basic metrics...")
            progress_bar.progress(0.25)
            
            # Vectorized calculations - Ï€Î¿Î»Ï Î³ÏÎ®Î³Î¿ÏÎ±!
            if sparse.issparse(adata.X):
                # Sparse matrix - optimized
                X_csr = adata.X.tocsr()
                
                # Basic metrics ÏƒÎµ Î­Î½Î± Î²Î®Î¼Î±
                total_counts = np.array(X_csr.sum(axis=1)).flatten()
                n_genes = np.array((X_csr > 0).sum(axis=1)).flatten()
                
                progress_bar.progress(0.5)
                status_text.text("ğŸ§¬ Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ MT genes...")
                
                # MT genes
                if n_mt_genes > 0:
                    mt_counts = np.array(X_csr[:, mt_gene_pattern].sum(axis=1)).flatten()
                    pct_counts_mt = np.divide(mt_counts, total_counts, 
                                            out=np.zeros_like(mt_counts), 
                                            where=total_counts!=0) * 100
                else:
                    pct_counts_mt = np.zeros(adata.n_obs)
                
                progress_bar.progress(0.75)
                status_text.text("ğŸ”¬ Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Ribosomal genes...")
                
                # Ribosomal genes
                if n_ribo_genes > 0:
                    ribo_counts = np.array(X_csr[:, ribo_gene_pattern].sum(axis=1)).flatten()
                    pct_counts_ribo = np.divide(ribo_counts, total_counts, 
                                              out=np.zeros_like(ribo_counts), 
                                              where=total_counts!=0) * 100
                else:
                    pct_counts_ribo = np.zeros(adata.n_obs)
                    
            else:
                # Dense matrix
                total_counts = adata.X.sum(axis=1)
                n_genes = (adata.X > 0).sum(axis=1)
                
                progress_bar.progress(0.5)
                status_text.text("ğŸ§¬ Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ MT genes...")
                
                if n_mt_genes > 0:
                    mt_counts = adata.X[:, mt_gene_pattern].sum(axis=1)
                    pct_counts_mt = np.divide(mt_counts, total_counts, 
                                            out=np.zeros_like(mt_counts), 
                                            where=total_counts!=0) * 100
                else:
                    pct_counts_mt = np.zeros(adata.n_obs)
                
                progress_bar.progress(0.75)
                status_text.text("ğŸ”¬ Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Ribosomal genes...")
                
                if n_ribo_genes > 0:
                    ribo_counts = adata.X[:, ribo_gene_pattern].sum(axis=1)
                    pct_counts_ribo = np.divide(ribo_counts, total_counts, 
                                              out=np.zeros_like(ribo_counts), 
                                              where=total_counts!=0) * 100
                else:
                    pct_counts_ribo = np.zeros(adata.n_obs)
            
            progress_bar.progress(1.0)
            status_text.text("ğŸ’¾ Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½...")
            
            # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÏ„Î¿ adata.obs
            adata.obs['n_genes'] = n_genes.astype(np.int32)
            adata.obs['total_counts'] = total_counts.astype(np.float32)
            adata.obs['pct_counts_mt'] = pct_counts_mt.astype(np.float32)
            adata.obs['pct_counts_ribo'] = pct_counts_ribo.astype(np.float32)
            
            # Validation
            validation_results = {
                'n_genes': f"Min: {n_genes.min()}, Max: {n_genes.max()}, Mean: {n_genes.mean():.1f}",
                'total_counts': f"Min: {total_counts.min():.1f}, Max: {total_counts.max():.1f}, Mean: {total_counts.mean():.1f}",
                'pct_counts_mt': f"Min: {pct_counts_mt.min():.2f}%, Max: {pct_counts_mt.max():.2f}%, Mean: {pct_counts_mt.mean():.2f}%",
                'pct_counts_ribo': f"Min: {pct_counts_ribo.min():.2f}%, Max: {pct_counts_ribo.max():.2f}%, Mean: {pct_counts_ribo.mean():.2f}%"
            }
            
            # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ UI
            progress_bar.empty()
            status_text.empty()
            
            st.success("âœ… Vectorized QC calculation Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ Î³ÏÎ®Î³Î¿ÏÎ±!")
            
            with st.expander("ğŸ“Š Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ QC Metrics"):
                for metric, stats in validation_results.items():
                    st.text(f"{metric}: {stats}")
            
            return adata
            
        except Exception as e:
            progress_bar.empty()
            status_text.empty()
            st.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿ vectorized calculation: {str(e)}")
            raise
    
    def memory_efficient_plot_data(self, adata, plot_type: str, max_points: int = 10000):
        """Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î³Î¹Î± plots Î¼Îµ memory efficiency"""
        
        if adata.n_obs > max_points:
            st.info(f"ğŸ“Š Subsample {max_points} ÎºÏ…Ï„Ï„Î¬ÏÏ‰Î½ Î³Î¹Î± visualization")
            # Random subsample Î³Î¹Î± plotting
            indices = np.random.choice(adata.n_obs, max_points, replace=False)
            plot_adata = adata[indices, :].copy()
        else:
            plot_adata = adata.copy()
        
        return plot_adata
    
    def emergency_memory_cleanup(self):
        """ÎˆÎºÏ„Î±ÎºÏ„Î¿Ï‚ ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î¼Î½Î®Î¼Î·Ï‚ ÏŒÏ„Î±Î½ Ï†Ï„Î¬Î½Î¿Ï…Î¼Îµ ÏƒÏ„Î¿ ÏŒÏÎ¹Î¿"""
        
        memory_info = self.get_system_memory_info()
        
        if memory_info['percent'] > (self.emergency_cleanup_threshold * 100):
            st.warning("âš ï¸ ÎˆÎºÏ„Î±ÎºÏ„Î¿Ï‚ ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î¼Î½Î®Î¼Î·Ï‚...")
            
            # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Streamlit cache
            if hasattr(st, 'cache_data'):
                st.cache_data.clear()
            
            # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ session state
            for key in list(st.session_state.keys()):
                obj = st.session_state[key]
                if hasattr(obj, 'nbytes') and obj.nbytes > 50 * 1024 * 1024:  # >50MB
                    del st.session_state[key]
                    st.info(f"ğŸ—‘ï¸ Î‘Ï†Î±Î¯ÏÎµÏƒÎ· {key} Î±Ï€ÏŒ session")
            
            # Force garbage collection
            gc.collect()
            
            # Î•Ï€Î±Î½Î­Î»ÎµÎ³Ï‡Î¿Ï‚ Î¼Î½Î®Î¼Î·Ï‚
            new_memory_info = self.get_system_memory_info()
            freed_mb = (memory_info['process_mb'] - new_memory_info['process_mb'])
            
            if freed_mb > 0:
                st.success(f"âœ… Î•Î»ÎµÏ…Î¸ÎµÏÏÎ¸Î·ÎºÎ±Î½ {freed_mb:.1f} MB Î¼Î½Î®Î¼Î·Ï‚")
            
            return True
        return False
    
    def check_file_size_compatibility(self, file_size_mb: float) -> Dict[str, Any]:
        """ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÏƒÏ…Î¼Î²Î±Ï„ÏŒÏ„Î·Ï„Î±Ï‚ Î¼ÎµÎ³Î­Î¸Î¿Ï…Ï‚ Î±ÏÏ‡ÎµÎ¯Î¿Ï… Î¼Îµ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î· Î¼Î½Î®Î¼Î·"""
        
        memory_info = self.get_system_memory_info()
        
        # Î•ÎºÏ„Î¯Î¼Î·ÏƒÎ· Î¼Î½Î®Î¼Î·Ï‚ Ï€Î¿Ï… Î¸Î± Ï‡ÏÎµÎ¹Î±ÏƒÏ„ÎµÎ¯ (ÏƒÏ…Î½Ï„Î·ÏÎ·Ï„Î¹ÎºÎ® ÎµÎºÏ„Î¯Î¼Î·ÏƒÎ·)
        estimated_memory_needed = file_size_mb * 3  # 3x Î³Î¹Î± processing overhead
        
        compatibility = {
            'can_process': False,
            'requires_streaming': False,
            'suggested_chunk_size': self.chunk_size,
            'warnings': [],
            'recommendations': []
        }
        
        if estimated_memory_needed > memory_info['available_gb'] * 1024:
            compatibility['warnings'].append(
                f"âš ï¸ Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ ({file_size_mb:.1f}MB) Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï…Ï€ÎµÏÎ²ÎµÎ¯ Ï„Î· Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î· Î¼Î½Î®Î¼Î·"
            )
            compatibility['requires_streaming'] = True
            compatibility['suggested_chunk_size'] = max(100, int(self.chunk_size / 2))
        else:
            compatibility['can_process'] = True
        
        if file_size_mb > self.max_file_size_mb:
            compatibility['warnings'].append(
                f"ğŸ“ ÎœÎµÎ³Î¬Î»Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ ({file_size_mb:.1f}MB) - Ï€ÏÎ¿Ï„ÎµÎ¯Î½ÎµÏ„Î±Î¹ streaming mode"
            )
            compatibility['requires_streaming'] = True
        
        if memory_info['percent'] > 70:
            compatibility['recommendations'].append(
                "ğŸ§¹ Î£Ï…Î½Î¹ÏƒÏ„Î¬Ï„Î±Î¹ ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î¼Î½Î®Î¼Î·Ï‚ Ï€ÏÎ¹Î½ Ï„Î· Ï†ÏŒÏÏ„Ï‰ÏƒÎ·"
            )
        
        return compatibility
    
    def adaptive_chunk_processing(self, adata, process_func, **kwargs):
        """Î ÏÎ¿ÏƒÎ±ÏÎ¼Î¿ÏƒÏ„Î¹ÎºÎ® ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î¼Îµ chunks Î²Î¬ÏƒÎµÎ¹ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î·Ï‚ Î¼Î½Î®Î¼Î·Ï‚ Î¼Îµ Î²ÎµÎ»Ï„Î¹Ï‰Î¼Î­Î½Î· Î±Ï€ÏŒÎ´Î¿ÏƒÎ·"""
        
        memory_info = self.get_system_memory_info()
        dataset_size_mb = self.estimate_memory_usage(adata)
        
        # ÎˆÎ¾Ï…Ï€Î½Î· Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿Î³Î® chunk size Î²Î¬ÏƒÎµÎ¹ Ï€Î¿Î»Î»ÏÎ½ Ï€Î±ÏÎ±Î³ÏŒÎ½Ï„Ï‰Î½
        base_chunk_size = self.chunk_size
        
        # Î Î±ÏÎ¬Î³Î¿Î½Ï„Î±Ï‚ Î¼Î½Î®Î¼Î·Ï‚
        if memory_info['available_gb'] < 1:
            memory_factor = 0.1  # Î Î¿Î»Ï Î¼Î¹ÎºÏÎ¬ chunks
        elif memory_info['available_gb'] < 2:
            memory_factor = 0.25
        elif memory_info['available_gb'] < 4:
            memory_factor = 0.5
        elif memory_info['available_gb'] < 8:
            memory_factor = 0.75
        else:
            memory_factor = 1.0
        
        # Î Î±ÏÎ¬Î³Î¿Î½Ï„Î±Ï‚ Î¼ÎµÎ³Î­Î¸Î¿Ï…Ï‚ dataset
        if dataset_size_mb > 2000:  # >2GB
            size_factor = 0.5
        elif dataset_size_mb > 1000:  # >1GB
            size_factor = 0.75
        else:
            size_factor = 1.0
        
        # Î Î±ÏÎ¬Î³Î¿Î½Ï„Î±Ï‚ Î±ÏÎ¹Î¸Î¼Î¿Ï ÎºÏ…Ï„Ï„Î¬ÏÏ‰Î½
        if adata.n_obs > 50000:
            cell_factor = 0.5
        elif adata.n_obs > 20000:
            cell_factor = 0.75
        else:
            cell_factor = 1.0
        
        # Î£Ï…Î½Î¿Î»Î¹ÎºÏŒÏ‚ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚
        adaptive_chunk_size = int(base_chunk_size * memory_factor * size_factor * cell_factor)
        adaptive_chunk_size = max(50, min(adaptive_chunk_size, 2000))  # ÎŒÏÎ¹Î± Î±ÏƒÏ†Î±Î»ÎµÎ¯Î±Ï‚
        
        st.info(f"ğŸ”„ Adaptive chunking: {adaptive_chunk_size} ÎºÏÏ„Ï„Î±ÏÎ±/chunk (Memory: {memory_info['available_gb']:.1f}GB, Dataset: {dataset_size_mb:.1f}MB)")
        
        results = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        memory_text = st.empty()
        performance_metrics = {
            'start_time': time.time(),
            'chunks_processed': 0,
            'memory_cleanups': 0,
            'errors': 0
        }
        
        try:
            n_chunks = (adata.n_obs + adaptive_chunk_size - 1) // adaptive_chunk_size
            
            for i in range(n_chunks):
                start_idx = i * adaptive_chunk_size
                end_idx = min((i + 1) * adaptive_chunk_size, adata.n_obs)
                
                # Real-time memory monitoring
                current_memory = self.get_system_memory_info()
                memory_text.text(f"ğŸ’¾ Memory: {current_memory['percent']:.1f}% ({current_memory['process_mb']:.0f}MB process)")
                
                # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î¼Î½Î®Î¼Î·Ï‚ Ï€ÏÎ¹Î½ Î±Ï€ÏŒ ÎºÎ¬Î¸Îµ chunk
                if current_memory['percent'] > 85:
                    st.warning("âš ï¸ Î¥ÏˆÎ·Î»Î® Ï‡ÏÎ®ÏƒÎ· Î¼Î½Î®Î¼Î·Ï‚ - ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚...")
                    if self.emergency_memory_cleanup():
                        performance_metrics['memory_cleanups'] += 1
                    
                    # Î”Ï…Î½Î±Î¼Î¹ÎºÎ® Î¼ÎµÎ¯Ï‰ÏƒÎ· chunk size Î±Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹
                    if current_memory['percent'] > 90:
                        adaptive_chunk_size = max(50, adaptive_chunk_size // 2)
                        st.info(f"ğŸ”§ ÎœÎµÎ¯Ï‰ÏƒÎ· chunk size ÏƒÎµ {adaptive_chunk_size} Î³Î¹Î± Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î±")
                
                # Update progress Î¼Îµ Î»ÎµÏ€Ï„Î¿Î¼ÎµÏÎµÎ¯Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚
                progress = (i + 1) / n_chunks
                progress_bar.progress(progress)
                
                elapsed_time = time.time() - performance_metrics['start_time']
                if i > 0:
                    avg_time_per_chunk = elapsed_time / (i + 1)
                    estimated_remaining = avg_time_per_chunk * (n_chunks - i - 1)
                    status_text.text(f"Chunk {i+1}/{n_chunks} | ÎºÏÏ„Ï„Î±ÏÎ± {start_idx+1}-{end_idx} | ETA: {estimated_remaining:.1f}s")
                else:
                    status_text.text(f"Chunk {i+1}/{n_chunks} | ÎºÏÏ„Ï„Î±ÏÎ± {start_idx+1}-{end_idx}")
                
                try:
                    # Î•Î¾Î±Î³Ï‰Î³Î® chunk Î¼Îµ memory-safe Ï„ÏÏŒÏ€Î¿
                    chunk = adata[start_idx:end_idx, :].copy()
                    
                    # Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Ï„Î·Ï‚ ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·Ï‚ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚
                    chunk_result = process_func(chunk, chunk_index=i, **kwargs)
                    results.append(chunk_result)
                    performance_metrics['chunks_processed'] += 1
                    
                    # Memory cleanup Î¼ÎµÏ„Î¬ Î±Ï€ÏŒ ÎºÎ¬Î¸Îµ chunk
                    del chunk
                    gc.collect()
                    
                except Exception as e:
                    st.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿ chunk {i+1}: {str(e)}")
                    performance_metrics['errors'] += 1
                    
                    # Î ÏÎ¿ÏƒÏ€Î¬Î¸ÎµÎ¹Î± recovery Î¼Îµ Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎ¿ chunk
                    if adaptive_chunk_size > 100:
                        adaptive_chunk_size = max(50, adaptive_chunk_size // 2)
                        st.info(f"ğŸ”„ Retry Î¼Îµ Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎ¿ chunk size: {adaptive_chunk_size}")
                        continue
                    else:
                        # Î‘Î½ ÎºÎ±Î¹ Ï„Î± Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎ± chunks Î±Ï€Î¿Ï„Ï…Î³Ï‡Î¬Î½Î¿Ï…Î½, skip
                        results.append(None)
                        continue
                
                # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î±Î½ Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÏƒÏ„Î±Î¼Î±Ï„Î®ÏƒÎ¿Ï…Î¼Îµ
                final_memory = self.get_system_memory_info()
                if final_memory['percent'] > 95:
                    st.error("âŒ ÎšÏÎ¯ÏƒÎ¹Î¼Î± Ï‡Î±Î¼Î·Î»Î® Î¼Î½Î®Î¼Î· - Î´Î¹Î±ÎºÎ¿Ï€Î® ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚")
                    break
                    
        except Exception as e:
            st.error(f"âŒ ÎšÏÎ¯ÏƒÎ¹Î¼Î¿ ÏƒÏ†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±: {str(e)}")
            raise
        finally:
            # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ UI elements
            progress_bar.empty()
            status_text.empty()
            memory_text.empty()
            
            # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· performance metrics
            total_time = time.time() - performance_metrics['start_time']
            st.success(f"âœ… Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÏƒÎµ {total_time:.1f}s")
            
            with st.expander("ğŸ“Š Performance Metrics"):
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Chunks Processed", performance_metrics['chunks_processed'])
                with col2:
                    st.metric("Memory Cleanups", performance_metrics['memory_cleanups'])
                with col3:
                    st.metric("Errors", performance_metrics['errors'])
                
                if performance_metrics['chunks_processed'] > 0:
                    avg_time = total_time / performance_metrics['chunks_processed']
                    st.text(f"Average time per chunk: {avg_time:.2f}s")
        
        return results
    
    def cleanup_session_memory(self):
        """ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î¼Î½Î®Î¼Î·Ï‚ session"""
        
        # Clear Streamlit cache
        if hasattr(st, 'cache_data'):
            st.cache_data.clear()
        
        # Clear session state Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î¼ÎµÎ³Î¬Î»Î± objects
        for key in list(st.session_state.keys()):
            obj = st.session_state[key]
            if hasattr(obj, 'nbytes') and obj.nbytes > 100 * 1024 * 1024:  # >100MB
                del st.session_state[key]
        
        # Force garbage collection
        gc.collect()
        
        st.info("ğŸ§¹ Session memory ÎºÎ±Î¸Î±ÏÎ¯ÏƒÏ„Î·ÎºÎµ")

# Global instance
memory_manager = AdvancedMemoryManager()
