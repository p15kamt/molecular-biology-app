"""
ÎŸÎ»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î¿ Module Î³Î¹Î± Ï„Î·Î½ Î•Î½Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î¼Îµ Scanorama

Î‘Ï…Ï„ÏŒ Ï„Î¿ module Ï€ÎµÏÎ¹Î»Î±Î¼Î²Î¬Î½ÎµÎ¹ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯ÎµÏ‚ Ï€Î¿Ï… Î±Ï€Î±Î¹Ï„Î¿ÏÎ½Ï„Î±Î¹ Î³Î¹Î± Ï„Î·Î½
ÎµÎ½Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï€Î¿Î»Î»Î±Ï€Î»ÏÎ½ scRNA-seq datasets, ÏƒÏ…Î¼Ï€ÎµÏÎ¹Î»Î±Î¼Î²Î±Î½Î¿Î¼Î­Î½Î¿Ï… Ï„Î¿Ï… batch correction
Î¼Îµ Scanorama ÎºÎ±Î¹ Ï„Î·Î½ Î±Î¾Î¹Î¿Î»ÏŒÎ³Î·ÏƒÎ· Ï„Î·Ï‚ Ï€Î¿Î¹ÏŒÏ„Î·Ï„Î±Ï‚ Ï„Î·Ï‚ integration.


Î—Î¼ÎµÏÎ¿Î¼Î·Î½Î¯Î±: 2025
"""

import streamlit as st
import pandas as pd
import numpy as np
import scanpy as sc
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import io
import tempfile
import os
import sys
from pathlib import Path

# Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· utils directory ÏƒÏ„Î¿ path
sys.path.append(str(Path(__file__).parent.parent / "utils"))

try:
    from memory_utils import memory_monitor, display_memory_info
    from advanced_memory import memory_manager
except ImportError:
    def memory_monitor(func):
        return func
    def display_memory_info():
        pass

# Scanorama import Î¼Îµ error handling
try:
    import scanorama
    SCANORAMA_AVAILABLE = True
except ImportError:
    SCANORAMA_AVAILABLE = False

class DataIntegrationPageComplete:
    """ÎŸÎ»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î· ÎºÎ»Î¬ÏƒÎ· Î³Î¹Î± Ï„Î· ÏƒÎµÎ»Î¯Î´Î± ÎµÎ½Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½"""
    
    def __init__(self):
        self.datasets = {}
        self.integrated_data = None
        self.integration_method = "simple_concatenation"
        
    def render(self):
        """ÎšÎµÎ½Ï„ÏÎ¹ÎºÎ® Î¼Î­Î¸Î¿Î´Î¿Ï‚ Î³Î¹Î± Ï„Î·Î½ ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Î·Ï‚ ÏƒÎµÎ»Î¯Î´Î±Ï‚"""
        
        st.title("ğŸ”— Î•Î½Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½")
        st.markdown("### Batch correction ÎºÎ±Î¹ ÎµÎ½Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï€Î¿Î»Î»Î±Ï€Î»ÏÎ½ scRNA-seq datasets")
        
        if not SCANORAMA_AVAILABLE:
            st.warning("âš ï¸ Scanorama Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ ÎµÎ³ÎºÎ±Ï„ÎµÏƒÏ„Î·Î¼Î­Î½Î¿. Î§ÏÎ®ÏƒÎ· ÎµÎ½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ®Ï‚ Î¼ÎµÎ¸ÏŒÎ´Î¿Ï….")
        
        # Memory information
        display_memory_info()
        
        # Tabs Î³Î¹Î± Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯ÎµÏ‚
        tab1, tab2, tab3 = st.tabs([
            "ğŸ“ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· & Î Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹", 
            "ğŸ”„ Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Integration",
            "ğŸ“Š Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±"
        ])
        
        with tab1:
            self.render_setup()
            
        with tab2:
            self.render_execution()
            
        with tab3:
            self.render_results()
    
    def render_setup(self):
        """Î¦ÏŒÏÏ„Ï‰ÏƒÎ· datasets ÎºÎ±Î¹ ÏÏÎ¸Î¼Î¹ÏƒÎ· Ï€Î±ÏÎ±Î¼ÎµÏ„ÏÏÎ½"""
        
        st.header("ğŸ“ Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Datasets")
        
        # Î•Ï€Î¹Î»Î¿Î³Î® Î¼ÎµÎ¸ÏŒÎ´Î¿Ï… Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚
        upload_method = st.radio(
            "Î•Ï€Î¹Î»Î­Î¾Ï„Îµ Î¼Î­Î¸Î¿Î´Î¿ Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚:",
            ["Î§ÏÎ®ÏƒÎ· Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¼Î­Î½Ï‰Î½ Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½", "Î‘Î½Î­Î²Î±ÏƒÎ¼Î± Î‘ÏÏ‡ÎµÎ¯Ï‰Î½"],
            horizontal=True
        )
        
        if upload_method == "Î§ÏÎ®ÏƒÎ· Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¼Î­Î½Ï‰Î½ Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½":
            # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÏƒÏ„Î¿ session state (multiple possible keys)
            preprocessed_data = None
            
            # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³Î¹Î± Î´Î¹Î¬Ï†Î¿ÏÎ± Ï€Î¹Î¸Î±Î½Î¬ keys
            if 'preprocessed_adata' in st.session_state and st.session_state['preprocessed_adata'] is not None:
                preprocessed_data = st.session_state['preprocessed_adata']
            elif 'adata' in st.session_state and st.session_state.adata is not None:
                preprocessed_data = st.session_state.adata
            
            if preprocessed_data is not None:
                st.info("ğŸ“Š Î’ÏÎ­Î¸Î·ÎºÎ±Î½ Ï€ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¼Î­Î½Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î±Ï€ÏŒ Ï„Î¿ Preprocessing module")
                
                # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¹ÏÎ½
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("ÎšÏÏ„Ï„Î±ÏÎ±", f"{preprocessed_data.n_obs:,}")
                with col2:
                    st.metric("Î“Î¿Î½Î¯Î´Î¹Î±", f"{preprocessed_data.n_vars:,}")
                
                # Î‘Î¥Î¤ÎŸÎœÎ‘Î¤Î— Ï€ÏÎ¿ÏƒÎ¸Î®ÎºÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ - Ï€Î¹Î¿ user-friendly
                if 'preprocessed_data' not in self.datasets:
                    self.datasets['preprocessed_data'] = preprocessed_data.copy()
                    st.success("âœ… Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¼Î­Î½Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î±!")
                else:
                    st.info("âœ… Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¼Î­Î½Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î®Î´Î· Ï†Î¿ÏÏ„Ï‰Î¼Î­Î½Î±")
                
                # Option Î³Î¹Î± manual reload
                if st.button("ğŸ”„ Reload Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¼Î­Î½Ï‰Î½ Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½"):
                    self.datasets['preprocessed_data'] = preprocessed_data.copy()
                    st.success("âœ… Î”ÎµÎ´Î¿Î¼Î­Î½Î± Î±Î½Î±Î½ÎµÏÎ¸Î·ÎºÎ±Î½!")
            else:
                st.warning("âš ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Ï€ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¼Î­Î½Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±.")
                st.info("ğŸ’¡ Î•ÎºÏ„ÎµÎ»Î­ÏƒÏ„Îµ Ï€ÏÏÏ„Î± Ï„Î¿ Preprocessing ÏƒÏ„Î· ÏƒÎµÎ»Î¯Î´Î± 'Î ÏÎ¿ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½'")
        
        else:
            # Multiple file uploader
            uploaded_files = st.file_uploader(
                "Î•Ï€Î¹Î»Î­Î¾Ï„Îµ H5AD Î±ÏÏ‡ÎµÎ¯Î±:",
                type=['h5ad'],
                accept_multiple_files=True
            )
            
            if uploaded_files:
                for uploaded_file in uploaded_files:
                    dataset_name = uploaded_file.name.replace('.h5ad', '')
                    
                    if dataset_name not in self.datasets:
                        try:
                            with tempfile.NamedTemporaryFile(delete=False, suffix='.h5ad') as tmp_file:
                                tmp_file.write(uploaded_file.getvalue())
                                tmp_path = tmp_file.name
                            
                            with st.spinner(f"Î¦ÏŒÏÏ„Ï‰ÏƒÎ· {dataset_name}..."):
                                adata = sc.read_h5ad(tmp_path)
                                adata = memory_manager.optimize_adata_for_memory(adata)
                                self.datasets[dataset_name] = adata
                            
                            os.unlink(tmp_path)
                            st.success(f"âœ… Dataset '{dataset_name}' Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎµ!")
                            
                        except Exception as e:
                            st.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚ {dataset_name}: {str(e)}")
        
        # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï†Î¿ÏÏ„Ï‰Î¼Î­Î½Ï‰Î½ datasets
        if self.datasets:
            st.subheader("ğŸ“Š Î¦Î¿ÏÏ„Ï‰Î¼Î­Î½Î± Datasets")
            
            for dataset_name, adata in self.datasets.items():
                with st.expander(f"Dataset: {dataset_name}"):
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("ÎšÏÏ„Ï„Î±ÏÎ±", f"{adata.n_obs:,}")
                    with col2:
                        st.metric("Î“Î¿Î½Î¯Î´Î¹Î±", f"{adata.n_vars:,}")
                    with col3:
                        memory_size = memory_manager.estimate_memory_usage(adata)
                        st.metric("ÎœÎ­Î³ÎµÎ¸Î¿Ï‚", f"{memory_size:.1f} MB")
            
            # Î Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹ Integration
            st.subheader("âš™ï¸ Î Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹ Integration")
            
            col1, col2 = st.columns(2)
            
            with col1:
                available_methods = ["simple_concatenation"]
                if SCANORAMA_AVAILABLE and len(self.datasets) > 1:
                    available_methods.insert(0, "scanorama")
                
                self.integration_method = st.selectbox(
                    "ÎœÎ­Î¸Î¿Î´Î¿Ï‚ Integration:",
                    available_methods
                )
            
            with col2:
                self.intersection_only = st.checkbox(
                    "Î§ÏÎ®ÏƒÎ· Î¼ÏŒÎ½Î¿ ÎºÎ¿Î¹Î½ÏÎ½ Î³Î¿Î½Î¹Î´Î¯Ï‰Î½",
                    value=False,  # PRODUCTION: Î”Î¹Î±Ï„Î®ÏÎ·ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î³Î¿Î½Î¹Î´Î¯Ï‰Î½
                    help="Î”Î¹Î±Ï„Î®ÏÎ·ÏƒÎ· Î¼ÏŒÎ½Î¿ Ï„Ï‰Î½ Î³Î¿Î½Î¹Î´Î¯Ï‰Î½ Ï€Î¿Ï… Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ ÏƒÎµ ÏŒÎ»Î± Ï„Î± datasets (ÏƒÏ…Î½Î¹ÏƒÏ„Î¬Ï„Î±Î¹: False Î³Î¹Î± Ï€Î»Î®ÏÎ· Î±Î½Î¬Î»Ï…ÏƒÎ·)"
                )
    
    @memory_monitor
    def render_execution(self):
        """Î•ÎºÏ„Î­Î»ÎµÏƒÎ· integration"""
        
        st.header("ğŸ”„ Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Integration")
        
        if not self.datasets:
            st.warning("âš ï¸ Î¦Î¿ÏÏ„ÏÏƒÏ„Îµ Ï€ÏÏÏ„Î± datasets")
            return
        
        if len(self.datasets) < 1:
            st.warning("âš ï¸ Î§ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Ï„Î¿Ï…Î»Î¬Ï‡Î¹ÏƒÏ„Î¿Î½ 1 dataset")
            return
        
        # Î ÏÎ¿ÎµÏ€Î¹ÏƒÎºÏŒÏ€Î·ÏƒÎ·
        st.subheader("ğŸ“‹ Î ÏÎ¿ÎµÏ€Î¹ÏƒÎºÏŒÏ€Î·ÏƒÎ·")
        
        total_cells = sum(adata.n_obs for adata in self.datasets.values())
        common_genes = self.get_common_genes()
        total_genes = len(common_genes) if self.intersection_only and common_genes else max(adata.n_vars for adata in self.datasets.values())
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ ÎšÏÏ„Ï„Î±ÏÎ±", f"{total_cells:,}")
        with col2:
            st.metric("Î“Î¿Î½Î¯Î´Î¹Î±", f"{total_genes:,}")
        with col3:
            estimated_memory = sum(memory_manager.estimate_memory_usage(adata) for adata in self.datasets.values()) * 1.5
            st.metric("Î•ÎºÏ„Î¹Î¼ÏÎ¼ÎµÎ½Î· ÎœÎ½Î®Î¼Î·", f"{estimated_memory:.1f} MB")
        
        # ÎšÎ¿Ï…Î¼Ï€Î¯ ÎµÎºÏ„Î­Î»ÎµÏƒÎ·Ï‚
        if st.button("ğŸš€ Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Integration", type="primary"):
            try:
                with st.spinner("Î•ÎºÏ„Î­Î»ÎµÏƒÎ· integration..."):
                    self.integrated_data = self.perform_integration()
                
                if self.integrated_data is not None:
                    st.success("âœ… Integration Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!")
                    
                    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÏ„Î¿ session state
                    st.session_state['integrated_data'] = self.integrated_data
                    
                    # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÏÎ½
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("ÎšÏÏ„Ï„Î±ÏÎ±", f"{self.integrated_data.n_obs:,}")
                    with col2:
                        st.metric("Î“Î¿Î½Î¯Î´Î¹Î±", f"{self.integrated_data.n_vars:,}")
                
            except Exception as e:
                st.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ integration: {str(e)}")
    
    def get_common_genes(self):
        """Î•ÏÏÎµÏƒÎ· ÎºÎ¿Î¹Î½ÏÎ½ Î³Î¿Î½Î¹Î´Î¯Ï‰Î½"""
        if not self.datasets:
            return []
        
        common_genes = None
        for adata in self.datasets.values():
            if common_genes is None:
                common_genes = set(adata.var_names)
            else:
                common_genes &= set(adata.var_names)
        
        return list(common_genes) if common_genes else []
    
    def perform_integration(self):
        """Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Ï„Î·Ï‚ integration"""
        
        if len(self.datasets) == 1:
            # Î‘Î½ Î­Ï‡Î¿Ï…Î¼Îµ Î¼ÏŒÎ½Î¿ Î­Î½Î± dataset, Î±Ï€Î»Î¬ Ï„Î¿ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†Î¿Ï…Î¼Îµ
            dataset_name, adata = next(iter(self.datasets.items()))
            adata_copy = adata.copy()
            adata_copy.obs['dataset'] = dataset_name
            return adata_copy
        
        if self.integration_method == "scanorama" and SCANORAMA_AVAILABLE:
            return self.perform_scanorama_integration()
        else:
            return self.perform_simple_concatenation()
    
    def perform_scanorama_integration(self):
        """Integration Î¼Îµ Scanorama Î¼Îµ memory optimization"""
        
        st.info("ğŸ”¬ Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Scanorama integration...")
        
        # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î¼Î½Î®Î¼Î·Ï‚ Ï€ÏÎ¹Î½ Ï„Î·Î½ ÎµÎºÏ„Î­Î»ÎµÏƒÎ·
        memory_info = memory_manager.get_system_memory_info()
        total_cells = sum(adata.n_obs for adata in self.datasets.values())
        total_genes = max(adata.n_vars for adata in self.datasets.values())
        
        # Î•ÎºÏ„Î¯Î¼Î·ÏƒÎ· Î±Ï€Î±Î¹Ï„Î¿ÏÎ¼ÎµÎ½Î·Ï‚ Î¼Î½Î®Î¼Î·Ï‚ (ÏƒÏ…Î½Ï„Î·ÏÎ·Ï„Î¹ÎºÎ®)
        estimated_memory_gb = (total_cells * total_genes * 4) / (1024**3)  # 4 bytes per float32
        
        st.info(f"ğŸ“Š Dataset info: {total_cells:,} ÎºÏÏ„Ï„Î±ÏÎ±, {total_genes:,} Î³Î¿Î½Î¯Î´Î¹Î±")
        st.info(f"ğŸ’¾ Î•ÎºÏ„Î¹Î¼ÏÎ¼ÎµÎ½Î· Î¼Î½Î®Î¼Î·: {estimated_memory_gb:.1f}GB, Î”Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î·: {memory_info['available_gb']:.1f}GB")
        
        # Î Î‘ÎÎ¤Î‘ Ï‡ÏÎ®ÏƒÎ· simple concatenation Î³Î¹Î± production - Î´Î¹Î±Ï„Î·ÏÎµÎ¯ ÏŒÎ»Î± Ï„Î± Î³Î¿Î½Î¯Î´Î¹Î±
        st.warning("âš ï¸ ÎœÎµÎ³Î¬Î»Î¿ dataset - Ï‡ÏÎ®ÏƒÎ· simple concatenation Î³Î¹Î± Î´Î¹Î±Ï„Î®ÏÎ·ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î³Î¿Î½Î¹Î´Î¯Ï‰Î½")
        st.info("ğŸ’¡ Î“Î¹Î± batch correction, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Ï„Î¿ Visualization module Î¼ÎµÏ„Î¬ Ï„Î·Î½ concatenation")
        return self.perform_simple_concatenation()
    
    def _standard_scanorama_integration(self):
        """ÎšÎ±Î½Î¿Î½Î¹ÎºÎ® Scanorama integration"""
        
        # Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
        datasets_list = []
        genes_list = []
        
        for dataset_name, adata in self.datasets.items():
            if hasattr(adata.X, 'toarray'):
                X_dense = adata.X.toarray()
            else:
                X_dense = adata.X
            
            datasets_list.append(X_dense)
            genes_list.append(adata.var_names.tolist())
        
        # Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Scanorama
        integrated_data, genes = scanorama.integrate(
            datasets_list,
            genes_list,
            knn=20,
            sigma=15.0,
            alpha=0.1,
            batch_size=5000
        )
        
        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± AnnData object
        integrated_X = np.vstack(integrated_data)
        
        # Î£Ï…Î»Î»Î¿Î³Î® obs data
        obs_list = []
        for dataset_name, adata in self.datasets.items():
            obs_subset = adata.obs.copy()
            obs_subset['dataset'] = dataset_name
            obs_list.append(obs_subset)
        
        integrated_obs = pd.concat(obs_list, ignore_index=True)
        
        # Î”Î¹ÏŒÏÎ¸Ï‰ÏƒÎ· dimensions Î³Î¹Î± var data
        actual_n_genes = integrated_X.shape[1]
        if actual_n_genes != len(genes):
            st.warning(f"âš ï¸ Standard Scanorama dimension mismatch: {actual_n_genes} != {len(genes)}")
            if actual_n_genes <= len(genes):
                genes_subset = genes[:actual_n_genes]
            else:
                genes_subset = genes + [f"gene_{i}" for i in range(len(genes), actual_n_genes)]
            integrated_var = pd.DataFrame(index=genes_subset)
        else:
            integrated_var = pd.DataFrame(index=genes)
        
        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± AnnData
        integrated_adata = sc.AnnData(
            X=integrated_X,
            obs=integrated_obs,
            var=integrated_var
        )
        
        return memory_manager.optimize_adata_for_memory(integrated_adata)
    
    def _memory_efficient_scanorama(self):
        """Memory-efficient Scanorama integration Î³Î¹Î± Î¼ÎµÎ³Î¬Î»Î± datasets"""
        
        st.info("ğŸ”„ Memory-efficient integration - Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î´Î¹Î±ÏÎºÎ­ÏƒÎµÎ¹ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ¿...")
        
        # Î£Ï„ÏÎ±Ï„Î·Î³Î¹ÎºÎ­Ï‚ Î³Î¹Î± Î¼ÎµÎ¯Ï‰ÏƒÎ· Î¼Î½Î®Î¼Î·Ï‚:
        # 1. Subsampling Î³Î¹Î± integration
        # 2. Î§ÏÎ®ÏƒÎ· Î¼ÏŒÎ½Î¿ highly variable genes
        # 3. Chunked processing
        
        # Î’ÏÎ¯ÏƒÎºÎ¿Ï…Î¼Îµ ÎºÎ¿Î¹Î½Î¬ highly variable genes
        common_hvg = self._get_common_highly_variable_genes()
        if len(common_hvg) < 1000:
            st.warning(f"âš ï¸ Î›Î¯Î³Î± ÎºÎ¿Î¹Î½Î¬ HVG ({len(common_hvg)}). Î§ÏÎ®ÏƒÎ· top variable genes...")
            common_hvg = self._get_top_variable_genes(n_genes=2000)
        
        st.info(f"ğŸ§¬ Î§ÏÎ®ÏƒÎ· {len(common_hvg)} Î³Î¿Î½Î¹Î´Î¯Ï‰Î½ Î³Î¹Î± integration")
        
        # Subsampling datasets Î±Î½ ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»Ï Î¼ÎµÎ³Î¬Î»Î±
        max_cells_per_dataset = 5000
        subsampled_datasets = {}
        
        for dataset_name, adata in self.datasets.items():
            if adata.n_obs > max_cells_per_dataset:
                st.info(f"ğŸ“Š Subsample {dataset_name}: {adata.n_obs} â†’ {max_cells_per_dataset} ÎºÏÏ„Ï„Î±ÏÎ±")
                # Random subsample
                indices = np.random.choice(adata.n_obs, max_cells_per_dataset, replace=False)
                adata_sub = adata[indices, common_hvg].copy()
            else:
                adata_sub = adata[:, common_hvg].copy()
            
            subsampled_datasets[dataset_name] = adata_sub
        
        # Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Scanorama Î¼Îµ Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎ± datasets
        datasets_list = []
        genes_list = []
        
        for dataset_name, adata in subsampled_datasets.items():
            # Î§ÏÎ®ÏƒÎ· sparse matrices ÏŒÏ€Î¿Ï… ÎµÎ¯Î½Î±Î¹ Î´Ï…Î½Î±Ï„ÏŒ
            if hasattr(adata.X, 'toarray'):
                # ÎœÏŒÎ½Î¿ Î±Î½ ÎµÎ¯Î½Î±Î¹ Î±ÏÎºÎµÏ„Î¬ Î¼Î¹ÎºÏÏŒ
                if adata.X.nnz < 10**7:  # <10M non-zero elements
                    X_dense = adata.X.toarray().astype(np.float32)  # float32 Î±Î½Ï„Î¯ float64
                else:
                    st.warning(f"âš ï¸ {dataset_name} Ï€Î¿Î»Ï Î¼ÎµÎ³Î¬Î»Î¿ Î³Î¹Î± dense conversion - Ï‡ÏÎ®ÏƒÎ· simple concatenation")
                    return self.perform_simple_concatenation()
            else:
                X_dense = adata.X.astype(np.float32)
            
            datasets_list.append(X_dense)
            genes_list.append(adata.var_names.tolist())
        
        try:
            # Scanorama Î¼Îµ Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎ¿ batch_size
            integrated_data, genes = scanorama.integrate(
                datasets_list,
                genes_list,
                knn=15,        # ÎœÎ¹ÎºÏÏŒÏ„ÎµÏÎ¿ knn
                sigma=10.0,    # ÎœÎ¹ÎºÏÏŒÏ„ÎµÏÎ¿ sigma
                alpha=0.05,    # ÎœÎ¹ÎºÏÏŒÏ„ÎµÏÎ¿ alpha
                batch_size=1000  # Î Î¿Î»Ï Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎ¿ batch_size
            )
            
            # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± integrated dataset
            integrated_X = np.vstack(integrated_data)
            
            st.info(f"ğŸ” Debug: integrated_X shape: {integrated_X.shape}")
            st.info(f"ğŸ” Debug: genes Î±Ï€ÏŒ scanorama: {len(genes)}")
            
            # Obs data
            obs_list = []
            for dataset_name, adata in subsampled_datasets.items():
                obs_subset = adata.obs.copy()
                obs_subset['dataset'] = dataset_name
                obs_list.append(obs_subset)
            
            integrated_obs = pd.concat(obs_list, ignore_index=True)
            
            # Î£Î—ÎœÎ‘ÎÎ¤Î™ÎšÎŸ: Î¤Î¿ var DataFrame Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î­Ï‡ÎµÎ¹ Ï„Î¿Î½ Î¯Î´Î¹Î¿ Î±ÏÎ¹Î¸Î¼ÏŒ rows Î¼Îµ Ï„Î± columns Ï„Î¿Ï… X
            if integrated_X.shape[1] != len(genes):
                st.warning(f"âš ï¸ Dimension mismatch: X columns ({integrated_X.shape[1]}) != genes ({len(genes)})")
                # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î± genes Ï€Î¿Ï… ÎµÏ€Î­ÏƒÏ„ÏÎµÏˆÎµ Ï„Î¿ Scanorama
                actual_n_genes = integrated_X.shape[1]
                if actual_n_genes <= len(genes):
                    # ÎšÏÎ±Ï„Î¬Î¼Îµ Ï„Î± Ï€ÏÏÏ„Î± N genes
                    genes_subset = genes[:actual_n_genes]
                else:
                    # Î ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ dummy genes
                    genes_subset = genes + [f"gene_{i}" for i in range(len(genes), actual_n_genes)]
                
                integrated_var = pd.DataFrame(index=genes_subset)
                st.info(f"ğŸ”§ Î”Î¹ÏŒÏÎ¸Ï‰ÏƒÎ·: Î§ÏÎ®ÏƒÎ· {len(genes_subset)} Î³Î¿Î½Î¹Î´Î¯Ï‰Î½")
            else:
                integrated_var = pd.DataFrame(index=genes)
            
            # AnnData object Î¼Îµ Î¼ÏŒÎ½Î¿ Ï„Î± integrated genes
            integrated_adata_hvg = sc.AnnData(
                X=integrated_X,
                obs=integrated_obs,
                var=integrated_var
            )
            
            st.success(f"âœ… Scanorama integration Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ!")
            st.info(f"ğŸ“Š Integration results: {integrated_adata_hvg.n_obs:,} ÎºÏÏ„Ï„Î±ÏÎ±, {integrated_adata_hvg.n_vars:,} HVG")
            
            # ÎšÎ¡Î™Î£Î™ÎœÎŸ: Î¤ÏÏÎ± Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Ï€ÏÎ¿ÏƒÎ¸Î­ÏƒÎ¿Ï…Î¼Îµ Ï€Î¯ÏƒÏ‰ ÏŒÎ»Î± Ï„Î± Î¬Î»Î»Î± Î³Î¿Î½Î¯Î´Î¹Î±!
            integrated_adata_full = self._restore_all_genes_after_integration(
                integrated_adata_hvg, subsampled_datasets
            )
            
            return memory_manager.optimize_adata_for_memory(integrated_adata_full)
            
        except Exception as e:
            st.error(f"âŒ Scanorama integration Î±Ï€Î­Ï„Ï…Ï‡Îµ: {str(e)}")
            st.info("ğŸ”„ Fallback ÏƒÎµ simple concatenation...")
            return self.perform_simple_concatenation()
    
    def _get_common_highly_variable_genes(self):
        """Î•ÏÏÎµÏƒÎ· ÎºÎ¿Î¹Î½ÏÎ½ highly variable genes"""
        
        hvg_sets = []
        for adata in self.datasets.values():
            if 'highly_variable' in adata.var.columns:
                hvg_genes = adata.var_names[adata.var['highly_variable']].tolist()
                hvg_sets.append(set(hvg_genes))
        
        if hvg_sets:
            # Intersection ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ sets
            common_hvg = list(set.intersection(*hvg_sets))
            return common_hvg
        else:
            return []
    
    def _get_top_variable_genes(self, n_genes=2000):
        """Î•ÏÏÎµÏƒÎ· top variable genes Î±Ï€ÏŒ ÏŒÎ»Î± Ï„Î± datasets"""
        
        all_genes = set()
        for adata in self.datasets.values():
            all_genes.update(adata.var_names.tolist())
        
        # Î•Ï€Î¹Î»Î¿Î³Î® Ï„Ï…Ï‡Î±Î¯Ï‰Î½ Î³Î¿Î½Î¹Î´Î¯Ï‰Î½ (Î±Ï€Î»Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î· Ï€ÏÎ¿ÏƒÎ­Î³Î³Î¹ÏƒÎ·)
        all_genes = list(all_genes)
        if len(all_genes) > n_genes:
            return np.random.choice(all_genes, n_genes, replace=False).tolist()
        else:
            return all_genes
    
    def _restore_all_genes_after_integration(self, integrated_adata_hvg, subsampled_datasets):
        """Î•Ï€Î±Î½Î±Ï†Î¿ÏÎ¬ ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î³Î¿Î½Î¹Î´Î¯Ï‰Î½ Î¼ÎµÏ„Î¬ Ï„Î· Scanorama integration"""
        
        st.info("ğŸ”„ Î•Ï€Î±Î½Î±Ï†Î¿ÏÎ¬ ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î³Î¿Î½Î¹Î´Î¯Ï‰Î½ Î³Î¹Î± Ï€Î»Î®ÏÎ· Î±Î½Î¬Î»Ï…ÏƒÎ·...")
        
        # Î’ÏÎ¯ÏƒÎºÎ¿Ï…Î¼Îµ ÏŒÎ»Î± Ï„Î± Î³Î¿Î½Î¯Î´Î¹Î± Î±Ï€ÏŒ Ï„Î± original datasets
        all_genes = set()
        for adata in self.datasets.values():
            all_genes.update(adata.var_names.tolist())
        all_genes = sorted(list(all_genes))
        
        st.info(f"ğŸ§¬ Î•Ï€Î±Î½Î±Ï†Î¿ÏÎ¬ {len(all_genes):,} ÏƒÏ…Î½Î¿Î»Î¹ÎºÏÎ½ Î³Î¿Î½Î¹Î´Î¯Ï‰Î½")
        
        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Î½Î­Î¿ X matrix Î¼Îµ ÏŒÎ»Î± Ï„Î± Î³Î¿Î½Î¯Î´Î¹Î±
        n_cells = integrated_adata_hvg.n_obs
        n_genes_total = len(all_genes)
        
        # Initialize Î¼Îµ zeros (sparse matrix Î³Î¹Î± memory efficiency)
        from scipy import sparse
        integrated_X_full = sparse.csr_matrix((n_cells, n_genes_total), dtype=np.float32)
        
        # Î‘Î½Ï„Î¹Î³ÏÎ±Ï†Î® Ï„Ï‰Î½ integrated HVG values ÏƒÏ„Î¹Ï‚ ÏƒÏ‰ÏƒÏ„Î­Ï‚ Î¸Î­ÏƒÎµÎ¹Ï‚
        hvg_genes = integrated_adata_hvg.var_names.tolist()
        
        for i, gene in enumerate(hvg_genes):
            if gene in all_genes:
                gene_idx = all_genes.index(gene)
                integrated_X_full[:, gene_idx] = integrated_adata_hvg.X[:, i]
        
        # Î“Î¹Î± Ï„Î± non-HVG Î³Î¿Î½Î¯Î´Î¹Î±, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î± original values
        # (Ï‡Ï‰ÏÎ¯Ï‚ batch correction, Î±Î»Î»Î¬ Î´Î¹Î±Ï„Î·ÏÎ¿ÏÎ¼Îµ Ï„Î·Î½ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯Î±)
        progress_bar = st.progress(0)
        
        cell_offset = 0
        for dataset_name, adata_sub in subsampled_datasets.items():
            n_cells_dataset = adata_sub.n_obs
            
            # Update progress
            progress = (cell_offset + n_cells_dataset) / n_cells
            progress_bar.progress(min(progress, 1.0))
            
            # Î“Î¹Î± ÎºÎ¬Î¸Îµ Î³Î¿Î½Î¯Î´Î¹Î¿ Ï€Î¿Ï… Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ HVG
            for gene in adata_sub.var_names:
                if gene not in hvg_genes and gene in all_genes:
                    gene_idx_global = all_genes.index(gene)
                    gene_idx_local = adata_sub.var_names.tolist().index(gene)
                    
                    # Î‘Î½Ï„Î¹Î³ÏÎ±Ï†Î® values
                    integrated_X_full[cell_offset:cell_offset+n_cells_dataset, gene_idx_global] = \
                        adata_sub.X[:, gene_idx_local]
            
            cell_offset += n_cells_dataset
        
        progress_bar.empty()
        
        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï€Î»Î®ÏÎ¿Ï…Ï‚ integrated dataset
        integrated_adata_full = sc.AnnData(
            X=integrated_X_full,
            obs=integrated_adata_hvg.obs.copy(),
            var=pd.DataFrame(index=all_genes)
        )
        
        # Î‘Î½Ï„Î¹Î³ÏÎ±Ï†Î® metadata Î±Ï€ÏŒ Ï„Î¿ HVG dataset
        for key in integrated_adata_hvg.var.columns:
            # Initialize Î¼Îµ default values
            integrated_adata_full.var[key] = False if key == 'highly_variable' else 0.0
            
            # Set values Î³Î¹Î± Ï„Î± HVG genes
            for i, gene in enumerate(hvg_genes):
                if gene in all_genes:
                    gene_idx = all_genes.index(gene)
                    integrated_adata_full.var.iloc[gene_idx][key] = integrated_adata_hvg.var.iloc[i][key]
        
        st.success(f"âœ… Î•Ï€Î±Î½Î±Ï†Î¿ÏÎ¬ Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ: {integrated_adata_full.n_obs:,} ÎºÏÏ„Ï„Î±ÏÎ±, {integrated_adata_full.n_vars:,} Î³Î¿Î½Î¯Î´Î¹Î±")
        
        return integrated_adata_full

    def perform_simple_concatenation(self):
        """Î‘Ï€Î»Î® concatenation"""
        
        st.info("ğŸ”— Î•ÎºÏ„Î­Î»ÎµÏƒÎ· simple concatenation...")
        
        # Î•ÏÏÎµÏƒÎ· ÎºÎ¿Î¹Î½ÏÎ½ Î³Î¿Î½Î¹Î´Î¯Ï‰Î½ Î±Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹
        if self.intersection_only:
            common_genes = self.get_common_genes()
            if not common_genes and len(self.datasets) > 1:
                st.error("âŒ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÎºÎ¿Î¹Î½Î¬ Î³Î¿Î½Î¯Î´Î¹Î±")
                return None
        
        # Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± datasets
        processed_datasets = []
        
        for dataset_name, adata in self.datasets.items():
            adata_copy = adata.copy()
            
            # Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± Î³Î¹Î± ÎºÎ¿Î¹Î½Î¬ Î³Î¿Î½Î¯Î´Î¹Î±
            if self.intersection_only and common_genes and len(self.datasets) > 1:
                adata_copy = adata_copy[:, common_genes]
            
            # Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· dataset information
            adata_copy.obs['dataset'] = dataset_name
            
            processed_datasets.append(adata_copy)
        
        # Concatenation
        if len(processed_datasets) == 1:
            integrated_adata = processed_datasets[0]
        else:
            integrated_adata = sc.concat(processed_datasets, axis=0, join='outer')
        
        return memory_manager.optimize_adata_for_memory(integrated_adata)
    
    def render_results(self):
        """Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½"""
        
        st.header("ğŸ“Š Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Integration")
        
        if self.integrated_data is None:
            if 'integrated_data' in st.session_state:
                self.integrated_data = st.session_state['integrated_data']
            else:
                st.warning("âš ï¸ Î•ÎºÏ„ÎµÎ»Î­ÏƒÏ„Îµ Ï€ÏÏÏ„Î± integration")
                return
        
        # Î’Î±ÏƒÎ¹ÎºÎ­Ï‚ ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ­Ï‚
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ ÎšÏÏ„Ï„Î±ÏÎ±", f"{self.integrated_data.n_obs:,}")
        with col2:
            st.metric("Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ Î“Î¿Î½Î¯Î´Î¹Î±", f"{self.integrated_data.n_vars:,}")
        with col3:
            if 'dataset' in self.integrated_data.obs.columns:
                n_datasets = self.integrated_data.obs['dataset'].nunique()
                st.metric("Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Datasets", n_datasets)
        
        # Dataset distribution
        if 'dataset' in self.integrated_data.obs.columns:
            st.subheader("ğŸ“Š ÎšÎ±Ï„Î±Î½Î¿Î¼Î® ÎšÏ…Ï„Ï„Î¬ÏÏ‰Î½ Î±Î½Î¬ Dataset")
            
            dataset_counts = self.integrated_data.obs['dataset'].value_counts()
            
            fig = px.bar(
                x=dataset_counts.index,
                y=dataset_counts.values,
                labels={'x': 'Dataset', 'y': 'Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ ÎšÏ…Ï„Ï„Î¬ÏÏ‰Î½'},
                title="ÎšÎ±Ï„Î±Î½Î¿Î¼Î® ÎšÏ…Ï„Ï„Î¬ÏÏ‰Î½ Î±Î½Î¬ Dataset"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Visualization
        st.subheader("ğŸ“ˆ ÎŸÏ€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ·")
        
        viz_method = st.selectbox(
            "ÎœÎ­Î¸Î¿Î´Î¿Ï‚ Visualization:",
            ["PCA", "UMAP", "t-SNE"]
        )
        
        if st.button(f"Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± {viz_method} Plot"):
            try:
                with st.spinner(f"Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ {viz_method}..."):
                    
                    if viz_method == "PCA":
                        sc.tl.pca(self.integrated_data)
                        basis = 'pca'
                    elif viz_method == "UMAP":
                        sc.tl.pca(self.integrated_data)
                        sc.pp.neighbors(self.integrated_data)
                        sc.tl.umap(self.integrated_data)
                        basis = 'umap'
                    else:  # t-SNE
                        sc.tl.pca(self.integrated_data)
                        sc.tl.tsne(self.integrated_data)
                        basis = 'tsne'
                    
                    # Plot
                    fig, ax = plt.subplots(figsize=(10, 8))
                    
                    if 'dataset' in self.integrated_data.obs.columns:
                        sc.pl.embedding(
                            self.integrated_data, 
                            basis=basis, 
                            color='dataset',
                            ax=ax,
                            show=False,
                            frameon=False
                        )
                        ax.set_title(f'{viz_method} - Colored by Dataset')
                    
                    st.pyplot(fig)
                    
            except Exception as e:
                st.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î·Î½ Î¿Ï€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ·: {str(e)}")
        
        # Export
        st.subheader("ğŸ’¾ Export Data")
        
        export_format = st.selectbox("ÎœÎ¿ÏÏ†Î® Export:", ["H5AD", "CSV"])
        
        if st.button("ğŸ“¥ Download Data"):
            try:
                if export_format == "H5AD":
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.h5ad') as tmp_file:
                        self.integrated_data.write_h5ad(tmp_file.name)
                        
                        with open(tmp_file.name, 'rb') as f:
                            st.download_button(
                                label="ğŸ’¾ Download H5AD",
                                data=f.read(),
                                file_name="integrated_data.h5ad",
                                mime="application/octet-stream"
                            )
                        
                        os.unlink(tmp_file.name)
                
                else:  # CSV
                    if hasattr(self.integrated_data.X, 'toarray'):
                        X_dense = self.integrated_data.X.toarray()
                    else:
                        X_dense = self.integrated_data.X
                    
                    df = pd.DataFrame(
                        X_dense,
                        index=self.integrated_data.obs_names,
                        columns=self.integrated_data.var_names
                    )
                    
                    csv_buffer = io.StringIO()
                    df.to_csv(csv_buffer)
                    
                    st.download_button(
                        label="ğŸ’¾ Download CSV",
                        data=csv_buffer.getvalue(),
                        file_name="integrated_data.csv",
                        mime="text/csv"
                    )
                
            except Exception as e:
                st.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î¿ export: {str(e)}")
